{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db202f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def locate_and_split(filepath : str):\n",
    "    loader = PyPDFLoader(file_path=filepath)\n",
    "    docs = loader.load_and_split()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "    chunks = text_splitter.split_documents(documents=docs)\n",
    "    print(\"Chunking...\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = locate_and_split(r\"D:\\Project\\RAG\\docs\\NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
    "chunks[0].metadata['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "from operator import add\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "from HybridSearchRAG.model import GenModel\n",
    "from HybridSearchRAG.retriever import get_hybrid_retriever\n",
    "from HybridSearchRAG.prompts import GRADE_DOCS, FINAL_ANSWER\n",
    "\n",
    "llm = GenModel()\n",
    "retriever = get_hybrid_retriever()\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages : List[BaseMessage]\n",
    "    question : str\n",
    "    documents : List[Document]\n",
    "    relevance : str\n",
    "    result : str\n",
    "\n",
    "def retriever_node(state : AgentState) -> AgentState:\n",
    "    print(\"--- RETRIEVING DOCUMENTS ---\")\n",
    "    query = state['question']\n",
    "    result = retriever.invoke(query)\n",
    "    print(f\"--- RETRIEVED {len(result)} DOCUMENTS ---\")\n",
    "    return {\"documents\" : result}\n",
    "\n",
    "def grade_docs(state : AgentState) -> AgentState:\n",
    "    print(\"--- GRADING DOCUMENTS ---\")  \n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "    prompt = GRADE_DOCS.format_prompt(question=question, documents=docs)\n",
    "\n",
    "    result = llm.invoke(prompt)\n",
    "    if \"yes\" in result.lower():\n",
    "        print(\"--- GRADE: DOCUMENTS ARE RELEVANT ---\")\n",
    "        return {\"relevance\": \"YES\"}\n",
    "    else:\n",
    "        print(\"--- GRADE: DOCUMENTS ARE NOT RELEVANT ---\")\n",
    "        return {\"relevance\": \"NO\"}\n",
    "    \n",
    "def generation(state : AgentState) -> AgentState:\n",
    "    print(\"--- GENERATING ANSWER ---\")\n",
    "\n",
    "    query = state[\"question\"]\n",
    "    docs = state['documents']\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt = FINAL_ANSWER.format_prompt(context=formatted_docs, question=query)\n",
    "\n",
    "    result = llm.invoke(prompt)\n",
    "    print(\"--- ANSWER GENERATED ---\")\n",
    "\n",
    "    return {\"result\" : result}\n",
    "\n",
    "def should_continue(state : AgentState):\n",
    "    relevance = state[\"relevance\"]\n",
    "\n",
    "    if \"yes\" in relevance.lower().strip():\n",
    "        return \"CONTINUE\"\n",
    "    return \"END\"\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"RETRIEVER NODE\", retriever_node)\n",
    "graph.add_node(\"GRADE NODE\", grade_docs)\n",
    "graph.add_node(\"GENERATION NODE\", generation)\n",
    "\n",
    "graph.add_edge(START, \"RETRIEVER NODE\")\n",
    "graph.add_edge(\"RETRIEVER NODE\", \"GRADE NODE\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"GRADE NODE\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"CONTINUE\" : \"GENERATION NODE\",\n",
    "        \"END\" : END\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"GENERATION NODE\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    " \n",
    "\n",
    "text = [\n",
    "    \"AI is field of Computer Science\",\n",
    "    \"Langchain is Python Agentic Framework\"\n",
    "]\n",
    "\n",
    "gemini_model = OllamaEmbeddings(model=\"embeddinggemma:latest\")\n",
    "mxbai_model = OllamaEmbeddings(model=\"mxbai-embed-large:335m\")\n",
    "\n",
    "gemini_text_embeddings = gemini_model.embed_documents(text)\n",
    "mxbai_text_embeddings = mxbai_model.embed_documents(text)\n",
    "\n",
    "gemini_embeddings = gemini_model.embed_query(\"What is AI\")\n",
    "mxbai_embeddings = mxbai_model.embed_query(\"What is AI\")\n",
    "\n",
    "print(gemini_embeddings)\n",
    "print(mxbai_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73754b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "BASE_URL = os.getenv(\"BASE_URL\")\n",
    "def GenModel(name : str = \"gpt-oss-120b:free\", \n",
    "             temperature : float = 1.0):\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=name,\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        temperature=temperature,\n",
    "        base_url=BASE_URL,\n",
    "        default_headers={\n",
    "        \"HTTP-Referer\": \"http://localhost\",   \n",
    "        \"X-Title\": \"Agent for project\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c11d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenModel()\n",
    "print(model.invoke(\"What is AI\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cd6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
