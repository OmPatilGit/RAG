{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db202f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def locate_and_split(filepath : str):\n",
    "    loader = PyPDFLoader(file_path=filepath)\n",
    "    docs = loader.load_and_split()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "    chunks = text_splitter.split_documents(documents=docs)\n",
    "    print(\"Chunking...\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e9f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'producer': 'PyPDF2',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '',\n",
       " 'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       " 'publisher': 'Curran Associates, Inc.',\n",
       " 'language': 'en-US',\n",
       " 'created': '2017',\n",
       " 'eventtype': 'Poster',\n",
       " 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       " 'title': 'Attention is All you Need',\n",
       " 'date': '2017',\n",
       " 'moddate': '2018-02-12T21:22:10-08:00',\n",
       " 'published': '2017',\n",
       " 'type': 'Conference Proceedings',\n",
       " 'firstpage': '5998',\n",
       " 'book': 'Advances in Neural Information Processing Systems 30',\n",
       " 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       " 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       " 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin',\n",
       " 'lastpage': '6008',\n",
       " 'source': 'D:\\\\Project\\\\RAG\\\\docs\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf',\n",
       " 'total_pages': 11,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = locate_and_split(r\"D:\\Project\\RAG\\docs\\NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
    "chunks[0].metadata['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c23a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
